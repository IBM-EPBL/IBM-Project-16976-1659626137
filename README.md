## IBM-Project-16976-1659626137
# Real-Time Communication System Powered by AI for Specially Abled
### Domain : Artificial Intelligence
### Team ID : PNT2022TMID53121
### Team Details : 
Team Lead - Srivarsha Elangovan  
Team Member 1 - Srikanth S Iyer  
Team Member 2 - Vaseekaran S  
Team Member 3 - Vignesh G R  
### Problem Statement :
In our society, we have people with disabilities. The technology is developing day by day, but no significant developments are undertaken for the betterment of these people. Communications between deaf-dumb and a normal person has always been a challenging task. It is very difficult for deaf and dumb people to convey their message to normal people. Since normal people are not trained on hand sign language. In times of emergency, conveying their message is very difficult. The human hand has remained a popular choice to convey information in situations where other forms like speech cannot be used. Building a system with Voice Conversion combined with Hand Gesture Recognition and translation will be very useful to have a proper conversation between a normal person and an impaired person in any language.  
  
The concepts of Machine learning algorithms and Neural networks are used to implement a solution. A Convolution Neural network is used to create a model that is subsequently trained on different hand gestures available in the dataset (almost around a thousand of them). All the hand gestures are fed into the model which are then processed, trained, and segregated using a certain machine learning algorithm. A certain No. of records are taken aside to continuously train the model and the rest are used to evaluate the learning of the built model. Clustering algorithms are used to segregate gestures into groups based on the different type of attributes available for a hand gesture. This information subsequently gets converted to human-understandable language and speech is given as output. 
  
In the other way, the same process is repeated where the input is given as a set of text commands from the end users, and they get converted to recognized hand gestures by the learning model which are then displayed to the impaired people.  
  
A Web application is built which uses this model. This Web application enables deaf and dumb people to convey their information
